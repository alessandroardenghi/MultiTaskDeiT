model_name: 'MultiTaskDeiT_patch16_tiny'
experiment_name: 'classification_multilabels_240_finetune'
data_path: 'coco_colors'
verbose: true
img_size: 240
batch_size: 32
epochs: 100
lr : 0.001 # 0.0001 when backbone unfrozen, 0.001 when backbone frozen
weights: 'lossweights/weights_25_bins_sigma_3_coco.npy'
n_workers: 16


active_heads:
    classification: true
    coloring: false
    jigsaw: false

pretrained_info:
    checkpoint_type: 'torch_checkpoint'         # can either be 'torch_checkpoint' or 'timm_name'
    link: 'logs/classification_multilabels_240_frozen_20250520_1955/checkpoints/best_model.pth' # when checkpoint_type is 'timm_name' use 'deit_tiny_patch16_224'


pixel_shuffle_cfg:
    do: false
    steps: 1
    smoothing: false

jigsaw_cfg:
    use_cls_embeds: true
    n_jigsaw_patches: 3

classification_cfg:
    n_classes: 80

freeze_modules:
    cls_token: false
    pos_embed: false
    patch_embed: false
    blocks: false
    norm: false
    jigsaw_head: false
    coloring_decoder: false
    class_head: false


unfreeze_modules:
    cls_token: false
    pos_embed: false
    patch_embed: false
    blocks: false
    norm: false
    jigsaw_head: false
    coloring_decoder: false
    class_head: false




